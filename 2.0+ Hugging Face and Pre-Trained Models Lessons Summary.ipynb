{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "001bf65f-1613-45c9-aa55-be7fc92ddef3",
   "metadata": {},
   "source": [
    "## 2.5 Hugging Face Pipeline for Quick Prototyping\n",
    "\n",
    "### Uses:\n",
    "* Transformers\n",
    "* Pytorch\n",
    "\n",
    "### Topics Covered:\n",
    "* Pipelines\n",
    "* Pipelines in Hugging Face\n",
    "* Pipeline Tasks\n",
    "* Example Pipelines Including:\n",
    "    * Sentiment Analysis Pipeline\n",
    "    * Question Answering Pipeline\n",
    "    * Translation Pipeline\n",
    "    * Text Generation Pipeline\n",
    "    * Conversation Pipeline\n",
    "        * One using model and tokenizer import\n",
    "        * One using pipeline import\n",
    "\n",
    "### Syntax Segments Summary:\n",
    "(See notebook 2.5 for an in-depth conversation model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de3ee5-a475-45e6-95df-1977ef35e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# 'pipeline' class allows for easy use of pre-trained models for NLP tasks\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# AutoModelForCasualLM class allows the automatic loading of a pre-trained language model for casual language \n",
    "# AutoTokenizer class helps tokenize input text\n",
    "\n",
    "import torch\n",
    "# Library for manipulating tensors\n",
    "\n",
    "from transformers import Conversation\n",
    "# 'Conversation' class is used to represent and handle conversational data understandable by LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ac788-7e31-43a2-a663-00b93ec70a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline(task=\"sentiment-analysis\")\n",
    "# downloads the default model for sentiment analysis\n",
    "# without specifying a model, the default model for the specified task is downloaded\n",
    "\n",
    "model = pipeline(model = \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# downloads a specific model trained for sentiment analysis\n",
    "\n",
    "model = pipeline(task=\"translation_en_to_fr\")\n",
    "# translation_en_to_fr model translates english to french\n",
    "\n",
    "model = pipeline(model=\"gpt2\")\n",
    "# the gpt2 model used for text generation\n",
    "\n",
    "model = pipeline(\"conversational\", \"microsoft/DialoGPT-medium\", pad_token_id=50256)\n",
    "# a specific model trained for conversation between it and a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193eb17-03b0-41eb-841e-bd8c1318389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline(task=\"sentiment-analysis\")\n",
    "# downloads the default model for sentiment analysis\n",
    "# without specifying a model, the default model for the specified task is downloaded\n",
    "\n",
    "model = pipeline(model = \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# downloads a specific model trained for sentiment analysis\n",
    "\n",
    "comp = model(text)\n",
    "# model performs sentiment-analysis pipeline on the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee41bf-c2ae-4e02-80fb-4afa7351c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline(task=\"question-answering\")\n",
    "# downloads the default model for question answering\n",
    "\n",
    "answer = model(\n",
    "    question = \"What date is Christmas?\", \n",
    "    # pass a question for the model to answer\n",
    "    context = \"Christmas is on Monday, the 25th of December, 2023\"\n",
    "    # provide context for the model to search for the answer to the question in\n",
    ")\n",
    "# answer is a dictionary with the keys:\n",
    "# 'score' -> the model's certainty\n",
    "# 'start' -> the answer string's beginning index inside of the context string\n",
    "# 'end' -> the answer string's ending index inside of the context string\n",
    "# 'answer' -> a piece of text that answers the question.\n",
    "    # either pulled from the context or generated by the model based on the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb1866-24af-4c94-ac4d-768009e4f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DialoGPT>> \" + conversation.generated_responses[-1])\n",
    "# .generated_responses holds a list of the model's reponses\n",
    "    # [-1] accesses the most recent response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb746b2-f700-4c6d-ada9-d60731589b92",
   "metadata": {},
   "source": [
    "## 2.7 Evaluating a Sentiment Analysis Model\n",
    "\n",
    "### Uses:\n",
    "* Datasets\n",
    "* Transformers\n",
    "\n",
    "### Topics Covered:\n",
    "* IMDb\n",
    "* Working with datasets\n",
    "* Computing a sentiment model using accuracy metrics\n",
    "* Tests default, SST-2, and Tweets models\n",
    "\n",
    "### Syntax Segments Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20097bc-3a22-42dc-8462-359732b76f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "# load_dataset function loads datasets from the Hugging Face datasets repository\n",
    "# load_metric function loads evaluation metrics used for measuring the performance of NLP models\n",
    "\n",
    "from transformers import pipeline\n",
    "# pipeline function allows you to create a pipeline for a specific task\n",
    "\n",
    "import pandas as pd\n",
    "# used for data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2478f-eb8b-4f8a-9da3-e0a5f9ba71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tweets dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "# \"imbd\" = name of dataset being loaded from HuggingFace 'datasets' library\n",
    "# split parameter specifies which part of the dataset to load\n",
    "    # 'test' typically refers to a subset of datasets used to evaluate performance\n",
    "# contains the features 'text' and the corresponding 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e55f7-1d59-46d0-a1fa-638cf39ebd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "# creates a pandas dataframe using the given information\n",
    "\n",
    "all_texts = df[\"text\"].values.tolist()\n",
    "# df[\"texts\"] selects the \"texts\" column\n",
    "    # .values converts selected column into NumPy array\n",
    "        # .tolist() converts the NumPy array into Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379dde36-63c3-4b6f-8e1c-dd881decfae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentiments = model(all_texts, truncation=True, max_length=512)\n",
    "# Would take ~16.5 hrs for me to run\n",
    "# sentiment performs computations on all_texts list\n",
    "# truncation = True takes the max_length (number of tokens) from texts that are too long\n",
    "# max_length sets the maximum number of tokens that are allowed per text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbd285-a9c1-4edf-9a43-323b08a3c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = metric.compute(predictions=predictions, references=references)\n",
    "# .compute() function of metric object calculates the accuracy score -\n",
    "    # by comparing the predicted values to the reference (true/accurate) values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287f61b-d6ec-4ebd-941d-6e01d566733d",
   "metadata": {},
   "source": [
    "## 2.10 Semtic Search on Big Data\n",
    "\n",
    "### Uses:\n",
    "* NumPy\n",
    "* Scikit-Learn\n",
    "* Faiss\n",
    "\n",
    "### Topics Covered:\n",
    "*  What is Semantic Search?\n",
    "*  Using Faiss to speed up Semantic Search\n",
    "*  Generating random vectors\n",
    "*  Brute-force Semantic Search\n",
    "*  Semantic Search with Space-partitioning Index\n",
    "\n",
    "### Syntax Segments Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7de61-762b-4250-9f09-9357b1d4efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# used for manipulating data\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "# normalize function used for normalizing arrays or vectors\n",
    "\n",
    "import faiss\n",
    "# Facebook AI similarity Search\n",
    "# Effifient in similarity search and the clustering of dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0553e56d-8674-418e-9ec8-298df4e716b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "# sets seed for NumPy random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43672d17-4090-4620-b1d8-056d2f2f5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.random.random((number_of_vectors, num_dimensions)).astype('float32')\n",
    "# np.random.random() generates random numbers in a provided range\n",
    "# (vectors, dimensions) specifies the shape of the generated array\n",
    "# .astype('float32') method converts the data type of the generated array into float32\n",
    "    # a data type representing 32-bit floating-point numbers\n",
    "\n",
    "vectors = normalize(vectors)\n",
    "# normalizes vectors along an axis (a dimension)\n",
    "# This divides each dimension by the vectors euclidean norm. Think of this norm as the magnitude of the vector.\n",
    "    # The new, normalized (divided) vector has a magnitude of 1, but is going in the same direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f89585-db6d-417e-965f-7e6ccff52dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(num_dimensions)\n",
    "# Creates an IndexFlatL2 in a space with a specified number of dimensions\n",
    "# A flat index is an index where all values are stored without hierarchy \n",
    "    # (all vectors have the same level of priority)\n",
    "# L2 is a similarity metric that queries for nearest neighbors \n",
    "    # (similar vectors with the least distance from eachother)\n",
    "\n",
    "index.add(vectors)\n",
    "# fills the index with the vectors\n",
    "\n",
    "retrieved_vector = index.reconstruct(0)\n",
    "# .reconstruct retrieves a single data point from the index based on the provided position/index (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1efb8-dc79-4c41-918d-5bdf928f4633",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = np.random.random((1, num_dimensions)).astype('float32')\n",
    "# creates a single vectors with 512 dimensions\n",
    "\n",
    "# Here is a mathematic explaination of normalization\n",
    "query_vector = normalize(query_vector)\n",
    "# changes the vector's magnitude to the L2 norm while the direction stays the same\n",
    "# L2 norm is a measure of the magnitude of a vector \n",
    "    # OR\n",
    "    # |v| = sqrt((c1)^2 + (c2)^2 + ... + (cn)^2)\n",
    "    # where all c are components of the vector\n",
    "# L1 norm measures the absolute sum of all components in a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93977af5-7ded-48e4-bf89-ed1a04fde70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = index.search(query_vector, num_neighbors)\n",
    "# .search() performs a nearest neighbor search provided:\n",
    "    # A starting point (query_vector)\n",
    "    # A number of neighbors to find (4)\n",
    "# Finds 1st, 2nd, 3rd, and 4th closest vectors to the query_vector\n",
    "# Returns:\n",
    "    # the distances between the vector and the query_vector\n",
    "    # the index of each vectors inside of the IndexFlatL2 obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9764334-8bc4-4977-82f5-c56d190dbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "IVFindex = faiss.IndexIVFFlat(quantizer, num_dimensions, n_cells)\n",
    "# Inverted File with Flat Index\n",
    "    # partitions the vector space into smaller cells (or clusters) of vectors\n",
    "    # each cell contains a subset of vectors based on their proximity (or similarity)\n",
    "# The quantizer Index serves as a coarse quantizer\n",
    "    # which retrieves an initial candidate set, then searches within this set for nearest neighbors\n",
    "# num_dimensions needs to be specified to maintain vectors with the same dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9144fc2-48c8-404c-a46e-00b16d626d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "IVFindex.train(vectors)\n",
    "# trains the index using the vectors dataset\n",
    "# prepares the data structure for efficient search\n",
    "\n",
    "IVFindex.add(vectors)\n",
    "# adds the vectors to the index structure\n",
    "# the vectors are partitioned in cells based on the quantizer to be used for nearest neighbor searches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f38214-34f5-4cc3-a81f-bd78a55c3740",
   "metadata": {},
   "source": [
    "## 2.17 Question Answering\n",
    "\n",
    "### Uses:\n",
    "* Transformers\n",
    "\n",
    "### Topics Covered:\n",
    "* Question Answering (QA) Including:\n",
    "    * Question Answering Variants\n",
    "    * Question Answering Datasets (Benchmarks)\n",
    "* QA using Python\n",
    "* Fast QA using Python\n",
    "\n",
    "### Syntax Segments Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af824a7-b481-4a94-b657-22ab4b170283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# pipeline function allows you to create a pipeline for a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c3b95-197d-412d-be20-d23549d9360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = pipeline(\"question-answering\")\n",
    "qa_response = qa_model(question=question, context=context)\n",
    "# Model returns a dictionary containing keys:\n",
    "    # score: the confidence of the model in extracting the answer from the context\n",
    "    # start: the index of the character in the context that corresponds to the start of the extracted answer.\n",
    "    # end: the index of the character in the context that corresponds to the end of the extracted answer.\n",
    "    # answer: the text extracted from the context, which should contain the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de5f18-2838-4b42-ad82-1eff3175ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_pipe = pipeline(\"text-classification\", model=\"cross-encoder/ms-marco-TinyBERT-L-2\")\n",
    "rankings = passage_pipe(passages_with_question)\n",
    "# embeds each sentence in a list, which has been concatenated with the question in each sentence.\n",
    "# the sentence with the highest embedding will be the most relevant to the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24621efb-2e3e-4011-909e-c1ba2b943772",
   "metadata": {},
   "source": [
    "## 2.18 Text Summarization\n",
    "\n",
    "### Uses:\n",
    "* Transformers\n",
    "\n",
    "### Topics Covered:\n",
    "* Summarizing Text in Python\n",
    "\n",
    "### Syntax Segments Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124660fd-5bba-4bcd-b87b-9940769e32fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# pipeline function allows you to create a pipeline for a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e20447-e0e3-48e7-a544-d9af7b3eb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_model = pipeline(\"summarization\")\n",
    "resp = sum_model(text)[0]\n",
    "# returns a list of a dictionary with 1 key -> 'summary_text'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
