{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff78fffc-3809-441a-92c2-bb502b9cc2d3",
   "metadata": {},
   "source": [
    "## Credit\n",
    "\n",
    "Notes are taken from NLPlanet Practical NLP with Python course section 1.5 Stemming, Lemmatization, Stopwords, POS Tagging.\n",
    "* https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/05-tokenization-stemming-lemmatization\n",
    "\n",
    "Authored by Fabio Chiusano\n",
    "* https://medium.com/@chiusanofabio94\n",
    "\n",
    "**All quotes '' are sourced from the NLPlanet course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f892267",
   "metadata": {},
   "source": [
    "**Inflection**\n",
    "\n",
    "<u>Inflection:<u>\n",
    "* The modification of a word to express different grammatical categories such as tense, case, tone of voice, person, number, and gender.\n",
    "* Ex: eat, eats, ate\n",
    "    \n",
    "<u>Inflected Language:<u>\n",
    "* A language that contains words that are derived from another word as their use in speech changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4c163",
   "metadata": {},
   "source": [
    "**Stemming**\n",
    "\n",
    "<u>Stemming:<u>\n",
    "* The process of reducing inflected words to their stem (their base or root form).\n",
    "* Related words need to reduce to the same stem whether or not the stem is a word or a root.\n",
    "* Operates on a single word without knowledge of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce84275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/brandon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# Allows you to tokenize\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tokenizes based on words and punctuation\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "# Performs suffix stripping to produce stems\n",
    "# Applies algorithmic rules to generate stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b4a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "# PorterStemmer object used to stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0ebdfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program program program program\n"
     ]
    }
   ],
   "source": [
    "# Exmaple\n",
    "\n",
    "print(\n",
    "stemmer.stem(\"program\"), \n",
    "stemmer.stem(\"programming\"),\n",
    "stemmer.stem(\"programer\"),\n",
    "stemmer.stem(\"programmed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce57c42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'march', 'ant', 'navig', 'the', 'wave', 'grass', 'blade', '.']\n"
     ]
    }
   ],
   "source": [
    "# Using tokenization\n",
    "\n",
    "text = \"The marching ants navigate the waving grass blades.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Stem\n",
    "token_stems = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print(token_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e6f4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming other languages: Import\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Contains a family of stemmers for different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf790a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming other languages: Available languages\n",
    "\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5d90cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algoritm algoritm\n"
     ]
    }
   ],
   "source": [
    "# Stemming other languages: Example use\n",
    "\n",
    "stemmer = SnowballStemmer(\"portuguese\")\n",
    "# SnowballStemmer object set to stem words from the german language\n",
    "\n",
    "print(\n",
    "stemmer.stem(\"Algoritmo\"),\n",
    "stemmer.stem(\"algoritmos\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbf4bd",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "<u>Lemmatization:<u>\n",
    "* Used to group together the inflected forms of a word so that they can be analyzed as a single item (their lemma).\n",
    "* Reduces inflected words to their lemma (an existing word).\n",
    "* Can leverage context to find hte correct lemma of a word.\n",
    "* Slower than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29de3693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/brandon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/brandon/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/brandon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "# WordNet is a lexical database that tracks words and their relations\n",
    "nltk.download('omw-1.4')\n",
    "# Open Multilingual WordNet provides translations and word senses in multiple languages\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Model used for POS tagging (mentioned later)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Class tokenizes text into individual tokens (words)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Class used to reduce words to their lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bafdeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# WordNetLemmatizer object\n",
    "\n",
    "print(lemmatizer.lemmatize(\"software\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7686f",
   "metadata": {},
   "source": [
    "**Stopwords**\n",
    "\n",
    "<u>Stop-words:<u>\n",
    "* Words that are so common in all texts that they add little to no information for NLP tasks.\n",
    "* Usually include articles and prepositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf69d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/brandon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# A corpus of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# Imports stopwords from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cde863d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of stopwords in English: 179\n",
      "Examples:\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stop words\n",
    "\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "# Retrieves stopwords specific to English\n",
    "\n",
    "print(f\"\"\"\n",
    "Total number of stopwords in English: {len(english_stopwords)}\n",
    "Examples:\n",
    "{english_stopwords[:10]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8285142",
   "metadata": {},
   "source": [
    "**POS Tagging**\n",
    "\n",
    "<u>Part-Of-Speech Tagging:<u>\n",
    "* The act of inferring the part of speech of words in sentences based on context.\n",
    "* POS Tags are acronyms for the part of speech a word represents. Ex: NN (noun) or VB (verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b6bd082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('ca', 'MD'), (\"n't\", 'RB'), ('figure', 'VB'), ('this', 'DT'), ('out', 'RP')]\n",
      "[('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('easiest', 'JJS'), ('thing', 'NN'), ('ever', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "import nltk\n",
    "\n",
    "text = word_tokenize(\"I can't figure this out\")\n",
    "print(nltk.pos_tag(text))\n",
    "# Takes a list of tokens and returns the POS (part-of-speech) tag for each token (word or punctuation)\n",
    "\n",
    "text = word_tokenize(\"This is the easiest thing ever\")\n",
    "print(nltk.pos_tag(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
