{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df57c09-93c8-4ec1-ad1f-545e2a07bfe2",
   "metadata": {},
   "source": [
    "## 1.4 Statistical approaches and text classification with N-grams\n",
    "\n",
    "### Uses:\n",
    "* Pandas\n",
    "* Scikit-learn\n",
    "\n",
    "### Topics Covered:\n",
    "* Difference between Expert Systems and Statistical Approaches.\n",
    "* Text Classification with N-grams including:\n",
    "* Vectorization\n",
    "* N-Grams\n",
    "* Bag of Words\n",
    "* Sparse Matrix\n",
    "* Dense Matrix\n",
    "* Making a logistic regression model.\n",
    "* Model training with feature weights (unigrams and bigrams)\n",
    "\n",
    "### Syntax Segments Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d1b16-d52d-4572-955e-6cf2c8c0af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Used to show tables with dataframes\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# CountVectorizer class is used to vectorize texts by counting the occurrences of each word\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Class can be used for logistic regression machine learning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffb9c7-997e-4a61-a3fd-1a1dc774c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "# ngram_range=(min_n, max_n) considers a specified range of grams (unigrams in this line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc5669-f17b-4909-aa3f-a7a0133133ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(texts) \n",
    "# .fit() method counts words in texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a8c48-5f81-456a-bd6d-dc0fa2a31ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform(texts)\n",
    "# .transform() method converts texts into a sparse matrix of token counts (ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c25141-b658-4382-867e-f2d1332eb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams.todense()\n",
    "# .todense() method converts a sparse matrix into a dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b5742-cae3-4092-b818-d183cad5f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_\n",
    "# Contains a mapping of terms (ngrams) to their indicies in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934c8ad-210a-4cf4-bd0a-93cd3bd1a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_.items()\n",
    "# Retrieves the vocab dictionary of grams and their matrix indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1ee79-e5d2-44ed-8a6d-b54725756fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ngrams_matrix, columns=keys)\n",
    "# .DataFrame(data, index, columns) method creates a Pandas DataFrame\n",
    "# data as dictionary: keys represent column names, values represent data in columns\n",
    "# data as list or NumPy array: represents the the values in the DataFrame. Rows and Columns will be indexed by default\n",
    "# data as another DataFrame: can be used to create a new DataFrame based on the existing DataFrame's data\n",
    "# index: row labels. Default = integer index (0, 1, 2,...)\n",
    "# columns: defines column labels. Default = inferred from input data or integer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087f3a9-3323-466c-bc50-b8321320cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "# LogisticRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04c876-21f0-4706-8a6f-fea797e2d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(ngrams, labels)\n",
    "# .fit() calibrates the LogisticRegression model into a matrix with specified labels\n",
    "# ngrams typically represents the matrix obtained from the text data after applying the CountVectorizer method\n",
    "# row: coresponds to text document\n",
    "# column: count of particular unigram in the document\n",
    "# labels contains the corresponding target labels for each text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cdfb9-f140-492a-a963-ac13ed1d7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_[0]\n",
    "# model.coef_[0] retrieves the weights learned by the logistic regression model after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c6f72-75bd-4c21-a99e-22996602de3b",
   "metadata": {},
   "source": [
    "## 1.5 Stemming, Lemmatization, Stopwords, and POS Tagging\n",
    "\n",
    "### Uses:\n",
    "* NLTK\n",
    "\n",
    "### Topics Covered:\n",
    "* Inflected Language\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "* Stopwords\n",
    "* POS Tagging\n",
    "\n",
    "### Syntax Segments Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85d4fd-06e2-4ebe-8b85-c497f38f0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# Allows you to tokenize\n",
    "nltk.download('wordnet')\n",
    "# WordNet is a lexical database that tracks words and their relations\n",
    "nltk.download('omw-1.4')\n",
    "# Open Multilingual WordNet provides translations and word senses in multiple languages\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Model used for POS tagging\n",
    "nltk.download('stopwords')\n",
    "# A corpus of stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tokenizes based on words and punctuation\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "# Performs suffix stripping to produce stems\n",
    "# Applies algorithmic rules to generate stems\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Contains a family of stemmers for different languages\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Class used to reduce words to their lemma\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# Imports stopwords from the downloaded corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcf2d9-f6be-45e9-8a57-eca2582b88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "# PorterStemmer object used to stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a08168-cc8b-441d-8284-8cdfa4b03abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(text)\n",
    "# .stem() reduces a given word to their stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f1c81-ee40-47e6-80f5-bd0563d29d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(text)\n",
    "# .word_tokenize() tokenizes the given text and returns a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f16f90-10ae-4370-98c2-3bf4ac50bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SnowballStemmer.languages\n",
    "# Contains a tuple of available languages the class if capable of stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a8057-dd29-470e-8bd4-b780c5a1e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "SnowballStemmer(avail_language)\n",
    "# Creates a stemmer object for the specified language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc7315-276e-43fa-9b5d-820a5a52024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# WordNetLemmatizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4ff4f-9f86-4282-baa9-a8616a40fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words(language)\n",
    "# Retrieves stopwords specific to the given language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705fb32-3a76-4bc2-8ad5-d3c6c3dd0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(text)\n",
    "# Takes a list of tokens and returns the POS (part-of-speech) tag for each token (word or punctuation)\n",
    "# Returns as a list of tuples with 2 items each (token, POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e624034-614e-47e6-a485-804551e16247",
   "metadata": {},
   "source": [
    "## 1.9 Representing Texts as Vectors TF-IDF\n",
    "\n",
    "### Uses:\n",
    "* HuggingFace Hub\n",
    "* NLTK\n",
    "* Pandas\n",
    "* Plotly\n",
    "* Collections\n",
    "* Regex\n",
    "\n",
    "### Topics Covered:\n",
    "* Zipf's Law\n",
    "* Use Case: Medium Articles\n",
    "* HuggingFace Datasets\n",
    "* DataFrames\n",
    "* Plotly Visualizations\n",
    "* Use Case: Brown Corpus\n",
    "* Use Case: Stop Words\n",
    "* TF-IDF\n",
    "\n",
    "### Syntax Segments Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d1d1fb-d961-4fdd-b6b6-a8c24d813725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "# hf_hub_download used to download hugging face models\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# Allows you to tokenize\n",
    "nltk.download('gutenberg')\n",
    "# Downloads the Gutenberg dataset\n",
    "nltk.download('stopwords')\n",
    "# A corpus of stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "# Used for data manipulation and analysis\n",
    "\n",
    "import plotly.express as px\n",
    "# Used for visualizations\n",
    "\n",
    "from collections import Counter\n",
    "# Counts occurrences of items in an iterable\n",
    "\n",
    "import re\n",
    "# Regular Expressions used for manipulating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f4ed0-e5f6-4588-8851-f48be9781587",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\n",
    "    hf_hub_download(\n",
    "        \"fabiochiu/medium-articles\", # Repository\n",
    "        repo_type=\"dataset\", # Type of content being fetched\n",
    "        filename=\"medium_articles.csv\" # File to be downloaded\n",
    "    )\n",
    "    # Downloads dataset from the Hugging Face model hub\n",
    ")\n",
    "# Reads a csv file into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c93e82-0f4a-4f3b-a85b-8c1a93d1bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.shape\n",
    "# Contains a tuple with the size of a given dataframe (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07918d-02b2-4c02-9179-3a8b347b3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.sample(n=30000)\n",
    "# .sample() randomly selects a subset of n items from the given DataFrame\n",
    "# Used to speed up computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346273e9-1e03-424b-b4c0-c124140af248",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()\n",
    "# Displays first few rows of the given DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338337ef-7e57-4d38-a6db-1b576e4e3ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[column_name_key].values)\n",
    "# .values returns a NumPy array from the given DataFrame filled with the values from the given key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219e865-883b-4e5f-a2d3-c25086862df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter_obj.most_common(top_n)\n",
    "# .most_common(n) returns a list of tuples of the top n tokens starting with the highest recurrence\n",
    "# tuples -> (token, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96553577-f784-4ae1-83e3-07296f270b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()\n",
    "# retrieves available corpus file IDs from the Gutenberg dataset from the corpora provided by nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898e849c-636b-4779-97b7-55c72ef23c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.words(corpus_name)\n",
    "# retrieves the text from a specified corpus inside of the gutenberg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae174ab4-9009-4610-8f68-c2df35a94bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')\n",
    "# retrieves a list of all stopwords in the english language from nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b93d113-be68-4d04-9a27-5b2632df4fc3",
   "metadata": {},
   "source": [
    "## 1.12 Representing Text as Vectors with Word Embeddings\n",
    "\n",
    "### Uses:\n",
    "* Sentence Transformers\n",
    "* HuggingFace Hub\n",
    "* Pandas\n",
    "* Numpy\n",
    "* Scikit-learn\n",
    "* Plotly\n",
    "\n",
    "### Topics Covered:\n",
    "* Word Embeddings\n",
    "* Word Embedding models\n",
    "* Context-independent Embedding\n",
    "* Context-dependent Embedding\n",
    "* Pre-trained Models\n",
    "* Finetuning\n",
    "* Sentence Embeddings\n",
    "* Sentence Transformers\n",
    "* Cosine Similarity\n",
    "* MTEB\n",
    "* Analyzing Similarity\n",
    "* Plotly Visualizations\n",
    "* Datasets\n",
    "* DataFrames\n",
    "* Bag of Words vs Embeddings comparison\n",
    "\n",
    "### Syntax Segments Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d4312-82ae-4b07-9031-26cb74519fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "# SentenceTransformer class used for sentence embedding\n",
    "# util contains utility functions to support the SentenceTransformer\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "# Used for downloading files from the Hugging Face model hub\n",
    "\n",
    "import pandas as pd\n",
    "# Used for data manipulation and analysis\n",
    "\n",
    "import numpy as np\n",
    "# Used for numerical computing and working with array and matrices\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "# PCA (Principal Component Analysis) is a class\n",
    "# A technique for dimensionality reduce\n",
    "# Used to reduce high-dimensional data to its important information\n",
    "\n",
    "from sklearn.manifold import TSNE \n",
    "# TSNE (t-Distributed Stochastic Neighbor Embedding) is a class\n",
    "# Used for visualizing high-dimensional data\n",
    "\n",
    "# Visualization\n",
    "\n",
    "import plotly.express as px\n",
    "# High-level interface used for plots and charts\n",
    "\n",
    "import plotly.io as pio\n",
    "# Used for handling input and output for Plotly visualizations (including saving or displaying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389c497-1549-4690-8873-49566be8d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_name)\n",
    "# Uploads the specified pretraiened sentence embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc13fc2-c087-41db-b526-baeaf60836e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode(sentences, convert_to_tensor=True)\n",
    "# .encode() takes in a list of sentences and represents the given text numerically (embeddings)\n",
    "# convert_to_tensor=True converts embeddings to tensors (numerical representations suitable for computation in frameworks like PyTorch or TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b22f48f-a33f-49e0-a2a6-2435ad24246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape\n",
    "# returns a a list the number of embeddings and the number of dimensions for each vector\n",
    "# lsit -> [num_embeddings, dimensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9daca-f3f0-41fe-a28b-ca30e96cfd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.cos_sim(embedding1, embedding2)[0].item()\n",
    "# util.cos_sim(vector1, vector2) calculates the cosine similarity between two given vectors\n",
    "# Returns as a tensor\n",
    "# [0] Accesses the first item (tensor may only have 1 item)\n",
    "# .item() extracts the similarity score as a single floating-point number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ce9fb-9288-48e4-abe2-b0ba2ef283a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    df_articles[df_articles[\"tags\"].apply(lambda taglist: \"Data Science\" in taglist)][:200],\n",
    "    df_articles[df_articles[\"tags\"].apply(lambda taglist: \"Business\" in taglist)][:200]\n",
    "]).reset_index(drop=True)\n",
    "# pd.concat concatenates subsets into a single DataFrame\n",
    "# .reset_index(drop=True) resets the index of the resulting DataFrame to start from 0 and drops the previous index\n",
    "# df_articles[\"tags\"] refers to the \"tags\" column of the DataFrame\n",
    "# .apply applies a function to each element in the specified column\n",
    "# lambda function takes the article's taglist and returns True if the string is in the taglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87841a20-3ec5-4838-b44b-7a68a607408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_obj = PCA(n_components=n)\n",
    "# PCA() creates a PCA object that reduces the data to a specified number of dimensions\n",
    "# n_components=n reduces the data to n dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f069c1e-e1fd-4798-bdc6-e3d815e36a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_pca = PCA_obj.fit_transform(embeddings)\n",
    "# .fit_transform() fits the PCA model to the embeddings \n",
    "# and transforms it into a new set of data with reduced dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d14b2b-3773-4069-b758-b25ab052f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tsne = TSNE(n_components=n, perplexity=p).fit_transform(embeddings_pca)\n",
    "# TSNE() creates a t-SNE object that reduces the dimensions for visualization\n",
    "# n_components=n reduces the data to n dimensions\n",
    "# perplexity determines the number of neighbors to consider when reducing dimensionality\n",
    "# low perplexity values lead to a more local view of data\n",
    "# higher perplexity values lead to a more global view of data\n",
    "# .fit_transform() fits the t-SNE model to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946071db-9b64-4881-9817-a8d9fcf41345",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\n",
    "    \"x\": x_dimension, # x coords\n",
    "    \"y\": y_dimension, # y coords\n",
    "    \"title\": titles, # plot title\n",
    "    \"color\": [\"color\" for _ in x_dimension] # Appends the string \"color\" to each x_dimension element\n",
    "    # Arbitrary placeholder \n",
    "}) \n",
    "# pd.DataFrame(data={}) creates a pandas DataFrane with the given information\n",
    "# key -> column name\n",
    "# value -> each item has a row in its column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
