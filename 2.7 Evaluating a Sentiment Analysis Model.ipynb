{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f23ba5df-4905-436e-92f5-cc8c1d6acf88",
   "metadata": {},
   "source": [
    "## Credit\n",
    "\n",
    "Notes are taken from NLPlanet Practical NLP with Python course section 2.7 Evaluating a Sentiment Analysis Model\n",
    "* https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/07-evaluate-sentiment-analysis\n",
    "\n",
    "Authored by Fabio Chiusano\n",
    "* https://medium.com/@chiusanofabio94\n",
    "\n",
    "**All quotes '' are sourced from the NLPlanet course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a8a0b0-e7f1-4972-b121-83c59b64f5b3",
   "metadata": {},
   "source": [
    "## Evaluating a Model over IMDb \n",
    "\n",
    "<u>IMDb:</u>\n",
    "* (Internet Movie Database) is a wealth of information about movies, shows, actors, directors, and more.\n",
    "* The IMDb contains user-generated reviews and ratings used in a dataset for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d02482-8cfc-417e-99cd-99e39743285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install datasets library\n",
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a821dd-71a6-4ac3-bddf-ebf7c11f9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets import load_dataset, load_metric\n",
    "# load_dataset function loads datasets from the Hugging Face datasets repository\n",
    "# load_metric function loads evaluation metrics used for measuring the performance of NLP models\n",
    "from transformers import pipeline\n",
    "# pipeline function allows you to create a pipeline for a specific task\n",
    "import pandas as pd\n",
    "# used for data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548a51d4-f515-4791-b6f5-13789fd92fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c279da14d2394ad9bf5ace971ae37e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22719b5c62f1462890cf53452e6cebf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cfad797eff452daf45c43660e13f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51bc86fedcd404b9069d0378f78fc3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348050764a6a4815803deefe4bda4e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d60bc9913445dc88fcaaeadebbbfe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30eb89adb1c244f69458d81dd183401a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Download IMDb Dataset\n",
    "\n",
    "# Download tweets dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "# \"imbd\" = name of dataset being loaded from HuggingFace 'datasets' library\n",
    "# split parameter specifies which part of the dataset to load\n",
    "    # 'test' typically refers to a subset of datasets used to evaluate performance\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "077574b3-e477-4ad2-85c3-43574f1f3dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love sci-fi and am willing to put up with a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worth the entertainment value of a rental, esp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its a totally average film with a few semi-alr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STAR RATING: ***** Saturday Night **** Friday ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>First off let me say, If you haven't enjoyed a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I love sci-fi and am willing to put up with a ...      0\n",
       "1  Worth the entertainment value of a rental, esp...      0\n",
       "2  its a totally average film with a few semi-alr...      0\n",
       "3  STAR RATING: ***** Saturday Night **** Friday ...      0\n",
       "4  First off let me say, If you haven't enjoyed a...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dataset to pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab33b67b-58c2-44e1-b230-694344d31c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline and pre-trained sentiment model\n",
    "model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1a459-191e-46f7-9070-307c5db6476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sentiment of each tweet using the model.\n",
    "all_texts = df[\"text\"].values.tolist()\n",
    "# df[\"texts\"] selects the \"texts\" column\n",
    "    # .values converts selected column into NumPy array\n",
    "        # .tolist() converts the NumPy array into Python list\n",
    "\n",
    "all_sentiments = model(all_texts, truncation=True, max_length=512)\n",
    "# Would take ~16.5 hrs for me to run\n",
    "# sentiment performs computations on all_texts list\n",
    "# truncation = True takes the max_length (number of tokens) from texts that are too long\n",
    "# max_length sets the maximum number of tokens that are allowed per text\n",
    "\n",
    "df[\"prediction\"] = [0 if d[\"label\"] == \"NEGATIVE\" else 1 for d in all_sentiments]\n",
    "# modifies the \"prediction\" column in the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b90a506-b387-4d58-916d-526c002cfe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy (SST-2 Model)\n",
    "\n",
    "# load 'accuracy' metric from datasets library\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "# compute accuracy over test set\n",
    "prediction = df[\"prediction\"]\n",
    "references = df[\"label\"]\n",
    "score = metric.compute(predictions=predictions, references=references)\n",
    "# .compute() function of metric object calculates the accuracy score -\n",
    "    # by comparing the predicted values to the reference (true/accurate) values\n",
    "print(score) # 0.89072\n",
    "# This is the accuracy rating. Think of it as <score>% accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8edd6e-af59-4e18-bb46-b8b8e9989c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy (Tweets Model)\n",
    "\n",
    "# load pipeline and pre-trained sentiment model\n",
    "model = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "# Compute the sentiment of each tweet using the model\n",
    "\n",
    "all_texts = df[\"text\"].values.tolist()\n",
    "all_sentiments = model(all_texts, truncation=True, max_length=512)\n",
    "# Would take ~16.5 hrs for me to run\n",
    "df[\"prediction\"] = [0 if d[\"label\"] == \"negative\" else 1 for d in all_sentiments]\n",
    "\n",
    "# Compute accuracy over test set\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "predictions = df[\"prediction\"]\n",
    "references = df[\"label\"]\n",
    "score = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(score) # 0.80772"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4c136-5eda-4727-b676-f997fc237be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy (IMDb Model)\n",
    "\n",
    "# load pipeline and pre-trained sentiment model\n",
    "model = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=0)\n",
    "\n",
    "# Compute the sentiment of each tweet using the model\n",
    "\n",
    "all_texts = df[\"text\"].values.tolist()\n",
    "all_sentiments = model(all_texts, truncation=True, max_length=512)\n",
    "# Would take ~16.5 hrs for me to run\n",
    "df[\"prediction\"] = [0 if d[\"label\"] == \"NEGATIVE\" else 1 for d in all_sentiments]\n",
    "\n",
    "# Compute accuracy over test set\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "predictions = df[\"prediction\"]\n",
    "references = df[\"label\"]\n",
    "score = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(score) # 0.928"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621ca0de-c095-45bb-b279-dfe8f63f530e",
   "metadata": {},
   "source": [
    "## Using Pre-Trained Models vs Fine-Tuning Your Own Model\n",
    "\n",
    "<u>Question:</u>\n",
    "* When computing the setniment of movie reviews published on a personal site, should a pre-trained model be used? or would it be better to use a fine-tuned model using your own movie reviews?\n",
    "\n",
    "<u>Answer:</u>\n",
    "* It depends on:\n",
    "    * 'If you estimate that a ~4% improvement in accuracy on your data will bring more benefits than the costs of building your dataset and finetuning a model, then it is better to proceed with the finetuning.'\n",
    "    * 'Otherwise, it is best to use a pre-trained model and have a good-enough solution right away, wihtout the extra expenses.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
