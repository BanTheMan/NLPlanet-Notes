{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49810cb6-a19b-4640-ab3f-083b3b5e24c2",
   "metadata": {},
   "source": [
    "## Credit\n",
    "\n",
    "Notes are taken from NLPlanet Practical NLP with Python course section 2.5 Hugging Face Pipeline for Quick Prototyping\n",
    "* https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/05-huggingface-pipeline \n",
    "\n",
    "Authored by Fabio Chiusano\n",
    "* https://medium.com/@chiusanofabio94\n",
    "\n",
    "**All quotes '' are sourced from the NLPlanet course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b5d99-bdb7-4d53-9d99-6b547bfd016c",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "<u>Pipelines:</u>\n",
    "* A series of data processing steps executed in a specific order to achieve a result better suited for things like machine learning.\n",
    "* <u>Pipelines in Hugging Face:</u>\n",
    "    * Their pipeline library assists in using models for inferencing.\n",
    "    * Their pipelines are \"objects that abstract most of the complex code from the library, offering a simple API dedicated to sever tasks.\"\n",
    "        * Tasks include:\n",
    "            * Named Entity Recognition\n",
    "            * Masked Language Modeling\n",
    "            * Sentiment Analysis\n",
    "            * Feature Extraction\n",
    "            * Question Answering\n",
    "        * Source: https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "### Constructing a pipeline object\n",
    "\n",
    "The constructor takes a *task* argument as input. \n",
    "Each *task* value has a corresponding pipeline.\n",
    "The available *task* values are:\n",
    "* \"audio-classification\"\n",
    "* \"automatic-speech-recognition\"\n",
    "* \"conversational\"\n",
    "* \"feature-extraction\"\n",
    "* \"fill-mask\"\n",
    "* \"image-classification\"\n",
    "* \"question-answering\"\n",
    "* \"table-question-answering\"\n",
    "* \"text2text-generation\"\n",
    "* \"text-classification\" or alias: \"sentiment-analysis\"\n",
    "* \"text-generation\"\n",
    "* \"token-classification\" or alias: \"ner\"\n",
    "* \"translation\"\n",
    "* \"translation_xx_to_yy\"\n",
    "* \"summarization\"\n",
    "* \"zero-shot-classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10fc31a-1c90-4911-8f03-4ccf2c1a0963",
   "metadata": {},
   "source": [
    "## Example pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac4307c-4fbe-4584-b77d-fa2c11ea28ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers library\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f4e04a-cb99-4c4c-8c32-3df067e8d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline class\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74182ad4-eaf1-4672-a38a-29cf51f3fae2",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242f261b-943f-4a16-89f1-6734bc3d91ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0adbc045863411a940ec503a9d4a746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74962bc4560245479cab3e1704e3872c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae69a44612c408d867760aa756c9711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2089f144e7d64c25b5db354e6b38aff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model by task\n",
    "\n",
    "model = pipeline(task=\"sentiment-analysis\")\n",
    "# downloads a model for sentiment analysis\n",
    "# without specifying a model, the default model for the specified task is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4419d63c-1802-4fa2-b8bb-e9d5fbf8341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specified model \n",
    "\n",
    "model = pipeline(model = \"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9f030fb-4972-4f20-9df2-c64736ed5587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text:\n",
      "Happy New Years!\n",
      "\n",
      "Computation:\n",
      "[{'label': 'POSITIVE', 'score': 0.9998683929443359}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use for computation\n",
    "\n",
    "text = \"Happy New Years!\"\n",
    "\n",
    "comp = model(text)\n",
    "# model performs sentiment-analysis pipeline on the given text\n",
    "\n",
    "print(f\"\"\"\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Computation:\n",
    "{comp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf5a1637-edd2-4d9c-a797-94312fd9f83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Text:\n",
      "    I had a great Christmas\n",
      "    \n",
      "    Computation:\n",
      "    {'label': 'POSITIVE', 'score': 0.9998540878295898}\n",
      "    \n",
      "\n",
      "    Text:\n",
      "    I didn't get what I wanted for Christmas\n",
      "    \n",
      "    Computation:\n",
      "    {'label': 'NEGATIVE', 'score': 0.999136745929718}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Computing a list of text\n",
    "\n",
    "text = [\n",
    "    \"I had a great Christmas\",\n",
    "    \"I didn't get what I wanted for Christmas\"\n",
    "]\n",
    "comp = model(text)\n",
    "\n",
    "for i in range(len(text)):\n",
    "    print(f\"\"\"\n",
    "    Text:\n",
    "    {text[i]}\n",
    "    \n",
    "    Computation:\n",
    "    {comp[i]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0743eb05-1798-4dc0-b125-1bc82079c323",
   "metadata": {},
   "source": [
    "### Question Answering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78265ec5-7e12-4df7-9f99-45f297328542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0558c104d74e499987c109ed6ee147c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80eaffa006644876a2978685b63b9412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883cc742e7224834a72207934aae48d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2473d3b6324b4db526f899744f2f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622016dbcfd249a092ca2d077020ff35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = pipeline(task=\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9cd88a1-d17f-487e-970f-567049635fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Answer: \n",
      "{'score': 0.4008321166038513, 'start': 16, 'end': 50, 'answer': 'Monday, the 25th of December, 2023'}\n",
      "\n",
      "Start-End Indicies:\n",
      "16-50\n",
      "\n",
      "Answer to Question: \n",
      "Monday, the 25th of December, 2023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer = model(\n",
    "    question = \"What date is Christmas?\", \n",
    "    # pass a question for the model to answer\n",
    "    context = \"Christmas is on Monday, the 25th of December, 2023\"\n",
    "    # provide context for the model to search for the answer to the question in\n",
    ")\n",
    "print(f\"\"\"\n",
    "Raw Answer: \n",
    "{answer}\n",
    "\n",
    "Start-End Indicies:\n",
    "{answer['start']}-{answer['end']}\n",
    "\n",
    "Answer to Question: \n",
    "{answer['answer']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49eb876-179c-44df-b6e7-e0c50fc29108",
   "metadata": {},
   "source": [
    "### Translation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6613df6b-f5e6-42d3-827f-553c83d9693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate translation model\n",
    "model = pipeline(task=\"translation_en_to_fr\")\n",
    "# translation_en_to_fr model translates english to french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a038b882-633c-42f5-9f9b-49e51a77cf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Appelez-moi Père Nol'}]\n"
     ]
    }
   ],
   "source": [
    "# Translate a piece of text\n",
    "\n",
    "text = \"Call me Santa\"\n",
    "translation = model(text)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef2543-58d5-4099-a463-95d4beb3eaa2",
   "metadata": {},
   "source": [
    "### Text Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad384324-a4bf-4f4c-92a7-58464daee613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the GPT2 model\n",
    "model = pipeline(model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbc4182f-3642-4b4a-af9b-0b75beb08339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Superman is just one more thing that I don't want to see happen in DC Comics. Even if there wasn't something like the Joker that was really a comic and didn't seem like a bad idea, he wouldn't fit in the DC universe\"}]\n"
     ]
    }
   ],
   "source": [
    "incipit = \"Superman is\"\n",
    "answer = model(incipit)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b09840-e7f2-4264-bcc3-ec4222b9e0c2",
   "metadata": {},
   "source": [
    "### Conversation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b10383-0a98-434b-b844-3531094118b0",
   "metadata": {},
   "source": [
    "Based on example from https://huggingface.co/microsoft/DialoGPT-medium?text=Hey+my+name+is+Thomas%21+How+are+you%3F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ff36ee6-dd15-48b9-91cb-9fed7c5a0e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# AutoModelForCasualLM class allows the automatic loading of a pre-trained language model for casual language \n",
    "# AutoTokenizer class helps tokenize input text\n",
    "import torch\n",
    "# Library for manipulating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "20f5b9e9-8636-4067-8ce7-71c89c58bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d43a7e12-d145-4d8d-a08c-e65417beeb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Hello! :D\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: How are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm good, how are you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: I'm doing well, and you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm doing well too, how about you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: I already told you!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm so sorry!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> User: You are forgiven.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: I'm so sorry!\n"
     ]
    }
   ],
   "source": [
    "# Chatting with a model \n",
    "# A warning message is unavoidable with this piece of code\n",
    "\n",
    "for step in range(5):\n",
    "    # Take and process user input\n",
    "    user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    # tokenizer.encode() converts input text to token IDs\n",
    "    # input() takes user input\n",
    "    # tokenizer.eos_token is an end-of-sequence token and marks the end of a sequence/text when tokenizing\n",
    "    # return_tensors='pt' returns tokens as PyTorch tensor\n",
    "\n",
    "    # Add input to history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, user_input_ids], dim=-1) if step > 0 else user_input_ids\n",
    "    # torch.cat() concatenates tensors\n",
    "        # takes a sequence (like a list) of tensors and the dimension to concatenate along\n",
    "        # dimension: dim\n",
    "            # dim=0 concatenates tensors along its rows (same number of columns, adds more rows)\n",
    "            # dim=1 concatenates tensors along its columns (same number of rows, adds more columns)\n",
    "            # dim=-1 concatenates along the last dimension in a tensor (extends only the last dimension)\n",
    "            # Example: 2x3 and 2x1 tensors along dim 1 -> 2x4 tensor\n",
    "                # (2 rows, 4 columns per row)\n",
    "    # if statement sets bot_input_ids to user_input_ids if this is the first message\n",
    "\n",
    "    # Generate response\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    # model.generate() method generates new sequences of tokens based on provided input of token IDs\n",
    "        # max_length parameter sets the maximum length of token IDs in the generated sequence\n",
    "        # pad_token_id specifies the token ID used for padding\n",
    "            # adds the specified token to the end of sequences to reach a uniform length in each sequence\n",
    "    # tokenizer.eos_token_id is the token ID of the end_of_sequence token\n",
    "\n",
    "    # pretty print last output tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "    # {} contains the formatted text\n",
    "    # .format is string formatting\n",
    "    # tokenizer.decode() converts a sequence of token IDs back into proper text using the tokenizer's vocabulary\n",
    "        # skip_special_tokens instructs the decoder to skip special tokens such as padding or eos tokens\n",
    "    # .shape[-1] retrieves the number of tokens of the tensor \n",
    "    # chat_history_ids[:, bot_input_ids.shape[-1]:][0]\n",
    "        # first : selects all rows in the tensor\n",
    "        # .shape[-1]: selects columns from a specified index (.shap[-1] being an index)\n",
    "        # [0] accesses the first element of the sliced tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb6382-2c22-492e-b420-ff86fc316868",
   "metadata": {},
   "source": [
    "#### Using Conversation object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bf0cbc37-d365-4b29-8657-3d32b3c55241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "model = pipeline(\"conversational\", \"microsoft/DialoGPT-medium\", pad_token_id=50256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "915cbd51-5742-4b13-9673-93dda5e3bb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT>> Hello! :D\n"
     ]
    }
   ],
   "source": [
    "from transformers import Conversation\n",
    "\n",
    "conversation = Conversation(\"Hello!\")\n",
    "# creates Conversation object loaded with the initial user input\n",
    "\n",
    "conversation = model(conversation)\n",
    "# updates the conversation object with a generated response\n",
    "\n",
    "print(\"DialoGPT>> \" + conversation.generated_responses[-1])\n",
    "# .generated_responses holds a list of the model's reponses\n",
    "    # [-1] accesses the most recent response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
